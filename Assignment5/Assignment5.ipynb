{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master DSLS / Programming 3 / Assignment 5\n",
    "# Mapreduce & PySpark Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Goal\n",
    "\n",
    "The goal of this assignment is to read in a large dataset of protein annotation information and to manipulate, summarize and analyze it using PySpark Dataframes.\n",
    "\n",
    "Protein annotation is a branch of bioinformatics which classifies the different parts of a protein's structure based on both sequence and functional characteristics. For instance, it recognizes structural elements like trans-membrane helices, but also particular active sites (\"Serine Protease\") and also signal peptides (\"periplasmic membrane tag\"). The holy grail of this field is to use these different annotations of parts of the protein sequence, and to combine them to predict the function of the protein as a whole. (Without having to carry out actual experiments in the lab !)\n",
    "\n",
    "The subject is the output of the InterProScan protein annotation service [InterproScan online](http://www.ebi.ac.uk/interpro/), [NAR article](https://academic.oup.com/nar/article/49/D1/D344/5958491) Briefly, InterPROscan is a meta-annotator; it runs different protein function annotators in turn on an input amino-acid sequence FASTA file and collects the output of each, labelling them with a unique and consistent identifier; the \"InterPRO number\". I used this service to annotate all currently known prokaryotic (Bacteria, Archaea) genomes to investigate better methods of metagenomics sequence annotation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Deliverables\n",
    "\n",
    "You need to write a script called `assignment5.py` in your `Assignment5` folder in your `programming3` GitHub repository. This script takes as input an InterPROscan output file; you can test on the example data in the /data/dataprocessing/interproscan/all_bacilli.tsv file on assemblix2012 and assemblix2019. You should use the PySpark Dataframe interface to read in and manipulate this file. This file contains ~4,200,000 protein annotations. You need to use the PySpark dataframe functions to answer the following questions:\n",
    "1. How many distinct protein annotations are found in the dataset? I.e. how many distinc InterPRO numbers are there?\n",
    "2. How many annotations does a protein have on average?\n",
    "3. What is the most common GO Term found?\n",
    "4. What is the average size of an InterPRO feature found in the dataset?\n",
    "5. What is the top 10 most common InterPRO features?\n",
    "6. If you select InterPRO features that are almost the same size (within 90-100%) as the protein itself, what is the top10 then?\n",
    "7. If you look at those features which also have textual annotation, what is the top 10 most common word found in that annotation?\n",
    "8. And the top 10 least common?\n",
    "9. Combining your answers for Q6 and Q7, what are the 10 most commons words found for the largest InterPRO features?\n",
    "10. What is the coefficient of correlation ($R^2$) between the size of the protein and the number of features found?\n",
    "\n",
    "Your output should be a CSV file with 3 columns;\n",
    "1. in the first column the question number\n",
    "2. in the second column the answer(s) to the question\n",
    "3. in the third column the output of the scheduler's physical plan (using the `.explain()` PySpark method) as a string\n",
    "\n",
    "NB1: Make sure you use the /commons/conda environment\n",
    "NB2: Use only 16 threads maximum; `sc = SparkContext('local[16]')`\n",
    "NB3: Use the `csv` Python module to make the CSV file in \"excel\" format; this makes it easier to deal with the different answer types (number, string, list etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1ba701004498>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStructType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStructField\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIntegerType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStringType\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mFloatType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType,FloatType\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/commons/conda/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/06/14 18:12:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/06/14 18:12:51 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/06/14 18:12:51 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"Protein_accession\", StringType(), True),\n",
    "    StructField(\"Sequence_MD5_digest\", StringType(), True),\n",
    "    StructField(\"Sequence_length\", IntegerType(), True),\n",
    "    StructField(\"Analysis\", StringType(), True),\n",
    "    StructField(\"Signature_accession\", StringType(), True),\n",
    "    StructField(\"Signature_description\", StringType(), True),\n",
    "    StructField(\"Start_location\", IntegerType(), True),\n",
    "    StructField(\"Stop_location\", IntegerType(), True),\n",
    "    StructField(\"Score\", FloatType(), True),\n",
    "    StructField(\"Status\", StringType(), True),\n",
    "    StructField(\"Date\", StringType(), True),\n",
    "    StructField(\"InterPro_annotations_accession\", StringType(), True),\n",
    "    StructField(\"InterPro_annotations_description\", StringType(), True),\n",
    "    StructField(\"GO_annotations\", StringType(), True),\n",
    "    StructField(\"Pathways_annotations\", StringType(), True)])\n",
    "spark = SparkSession.builder.master(\"local[16]\").appName(\"InterPro\").getOrCreate()\n",
    "df = spark.read.option(\"sep\",\"\\t\").option(\"header\",\"False\").csv(\"/data/dataprocessing/interproscan/all_bacilli.tsv\",schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 1. How many distinct protein annotations are found in the dataset? I.e. how many distinc InterPRO numbers are there?\n",
    "data1 = df.select('InterPro_annotations_accession')\\\n",
    "          .filter(df.InterPro_annotations_accession != \"-\")\\\n",
    "          .distinct()\\\n",
    "          .count()\n",
    "# 2. How many annotations does a protein have on average?\n",
    "data2 = df.select(\"Protein_accession\",'InterPro_annotations_accession')\\\n",
    "            .filter(df.InterPro_annotations_accession != \"-\")\\\n",
    "            .groupBy(\"Protein_accession\")\\\n",
    "            .count()\\\n",
    "            .select(mean(\"count\"))\\\n",
    "            .collect()[0].__getitem__(0)\n",
    "# 3. What is the most common GO Term found?\n",
    "data3 = df.select(split(col(\"GO_annotations\"),\"\\|\")).filter(df.GO_annotations != \"-\").collect()\n",
    "ans = [];[ans.extend(data[0]) for data in data3]\n",
    "annotation_count={}\n",
    "for annotation in ans:\n",
    "    if annotation in annotation_count:\n",
    "        annotation_count[annotation] +=1\n",
    "    else:\n",
    "        annotation_count[annotation] = 1\n",
    "value1 = 0; data3=\"\"\n",
    "for key, value in annotation_count.items():\n",
    "    if value > value1:\n",
    "        value1 = value\n",
    "        data3 = key\n",
    "# 4. What is the average size of an InterPRO feature found in the dataset?\n",
    "data4 = df.withColumn('Sub', ( df['Stop_location'] - df['Start_location'])).summary(\"mean\").collect()[0].__getitem__(-1)\n",
    "# 5. What is the top 10 most common InterPRO features?\n",
    "data5 = df.select('InterPro_annotations_accession')\\\n",
    "            .filter(df.InterPro_annotations_accession != \"-\")\\\n",
    "            .groupBy('InterPro_annotations_accession')\\\n",
    "            .count()\\\n",
    "            .sort(\"count\",ascending=False)\\\n",
    "            .select(\"InterPro_annotations_accession\")\\\n",
    "            .collect()[:10]\n",
    "data5 = [data[0] for data in data5]\n",
    "# 6. If you select InterPRO features that are almost the same size (within 90-100%) as the protein itself, what is the top10 then?\n",
    "data6 = df.select('InterPro_annotations_accession',\"Sequence_length\")\\\n",
    "            .filter((df['Stop_location'] - df['Start_location'])>=0.9)\\\n",
    "            .filter(df.InterPro_annotations_accession != \"-\")\\\n",
    "            .groupBy('InterPro_annotations_accession')\\\n",
    "            .count()\\\n",
    "            .sort(\"count\",ascending=False)\\\n",
    "            .select(\"InterPro_annotations_accession\")\\\n",
    "            .collect()[:10]\n",
    "data6 = [data[0] for data in data6]\n",
    "# 7. If you look at those features which also have textual annotation, what is the top 10 most common word found in that annotation?\n",
    "\n",
    "# 8. And the top 10 least common?\n",
    "# 9. Combining your answers for Q6 and Q7, what are the 10 most commons words found for the largest InterPRO features?\n",
    "# 10. What is the coefficient of correlation ($R^2$) between the size of the protein and the number of features found?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data7 = df.select(split(col(\"InterPro_annotations_description\"),\" |,|\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------+\n",
      "|split(InterPro_annotations_description,  |,|, -1)|\n",
      "+-------------------------------------------------+\n",
      "|                             [B, a, c, t, e, r...|\n",
      "|                             [T, h, i, a, z, o...|\n",
      "|                             [Y, c, a, O, -, l...|\n",
      "|                                            [-, ]|\n",
      "|                                            [-, ]|\n",
      "|                                            [-, ]|\n",
      "|                             [Y, c, a, O, -, l...|\n",
      "|                                            [-, ]|\n",
      "|                             [Y, c, a, O, -, l...|\n",
      "|                                            [-, ]|\n",
      "|                             [T, r, a, n, s, c...|\n",
      "|                             [O, m, p, R, /, P...|\n",
      "|                             [O, m, p, R, /, P...|\n",
      "|                                            [-, ]|\n",
      "|                             [O, m, p, R, /, P...|\n",
      "|                             [O, m, p, R, /, P...|\n",
      "|                             [S, i, g, n, a, l...|\n",
      "|                             [W, i, n, g, e, d...|\n",
      "|                             [C, h, e, Y, -, l...|\n",
      "|                                            [-, ]|\n",
      "+-------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data7.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extra Information\n",
    "- The PySpark documentation; https://spark.apache.org/docs/latest/api/python/index.html\n",
    "- The InterProScan documentation; https://interpro-documentation.readthedocs.io/en/latest/interproscan.html\n",
    "- Nice Spark tutorial; http://ampcamp.berkeley.edu/big-data-mini-course/data-exploration-using-spark.html\n",
    "- About Protein annotation; Paper on PFAM, Paper on TIGRFAM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "vscode": {
   "interpreter": {
    "hash": "ca1b872015604588654afab8889c327752f46265fe55163fa640f2d3ca6aaea6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
